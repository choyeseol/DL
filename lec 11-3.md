# lec 11-3

## Contents

- LeNet-5
- AlexNet
  - First layer
  - Second layer
- GoogLeNet
- ResNet (Residual Network)

ㅤ

ㅤ

ㅤ

오늘은 CNN이 발전해온 과정에 대해 알아볼 것이다.

# LeNet-5

가장 처음 CNN을 구성한 분은 LeCun 교수님이다.  
이 LeNet-5는 6개의 hidden layer를 사용하고 있다.

1. Input
2. c1
3. S2
4. c3
5. s4
6. c5
7. f6
8. Output

ㅤ

ㅤ

ㅤ

ㅤ

# AlexNet

2012년 `ImageNet`에서 우승한 AlexNet,  
크기가 큰 컬러 이미지를 입력받는다.

![Alt text](image.png)

ㅤ

ㅤ

## First layer

first layer는 96개의 11x11 크기의 필터를 stride 4로 사용한다.  
결과는 55x55x96으로 파라미터의 크기는 35k이다.

![Alt text](image-1.png)

ㅤ

ㅤ

## Second layer

second layer는 Pooling을 사용한다.  
3x3 크기의 필터로, stride 2를 사용한다.  
결과는 27x27x96으로, max pooling이기에 필요한 변수는 없다.

![Alt text](image-2.png)

어떻게 동작하는지 궁금하면 아래 그림을 참고하면 된다.

![Alt text](image-4.png)

레이어 중간에 `Normalization layer`가 있는데, 최근들어 사용하지 않는다.
마지막 값에는 fully connect된 NN을 넣는다.

더 디테일한 것은

- ReLu를 사용한 첫번째 모델이다.
- Dropout을 사용했다.
- 7개의 모델을 만들어 앙상블했다.
  등등으로 오른쪽의 파랗게 적힌 부분을 보면 알 수 있다.

1개의 모델에서는 18%의 에러를 갖지만, 여러개를 합하면 15%로 줄어든다.

ㅤ

ㅤ

ㅤ

ㅤ

# GoogLeNet

그 다음 나온 모델은 GoogLeNet이다.
시초, 발단이라는 뜻의 Inception module을 적용했다.
새로운 이론인 것이다.

2014, 2015년도 우승작으로, 사람이 판단하는 5%의 에러보다 낮은 3% 에러를 달성했다.

![Alt text](image-5.png)

Inception module에는 hidden layer에 포함된 노드가 7개 있는데, 최대 2개만 거치면 목표를 달성할 수 있다.

ㅤ

ㅤ

ㅤ

ㅤ

# ResNet (Residual Network)

갈 수록 레이어가 깊어진다. 레이어가 깊어지면 학습하는데 어려움을 겪는데,  
이를 해결하기 위해 ReLu와 Dropout를 사용했다.

ResNet은 기존의 모델과 같이 모든 layer를 거치는 것이 아닌, 건너뛰어 일부 layer만 거치는 것이다.

![Alt text](image-6.png)

여러 layer를 건너 뛰어서 목표를 금방 달성하기에 fast net이라고도 한다.

ㅤ

ㅤ

ResNet과 GoogLeNet에는 공통점이 있다.  
목표를 당성하기 위해 거치는 layer의 갯수가 적다는 것이다.

![Alt text](image-7.png)

GoogLeNet은 경로가 여러개이지만 특정 경로로 진입하면 경로에 포함된 모든 layer를 거쳐야한다는 것이다.  
RecNet이 GoogLeNet보다 균형 잡힌 결과를 만들어낼 수 있는 구조인 것이다.
